{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "\n",
    "from supcon import losses\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from math import log2, ceil\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from proglearn.progressive_learner import ProgressiveLearner\n",
    "from proglearn.deciders import SimpleArgmaxAverage\n",
    "from proglearn.transformers import NeuralClassificationTransformer, TreeClassificationTransformer\n",
    "from proglearn.voters import TreeClassificationVoter, KNNClassificationVoter\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import sys\n",
    "\n",
    "#%%\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def get_size(obj, seen=None):\n",
    "    \"\"\"Recursively finds size of objects\"\"\"\n",
    "    size = sys.getsizeof(obj)\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    # Important mark as seen *before* entering recursion to gracefully handle\n",
    "    # self-referential objects\n",
    "    seen.add(obj_id)\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum([get_size(v, seen) for v in obj.values()])\n",
    "        size += sum([get_size(k, seen) for k in obj.keys()])\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_size(obj.__dict__, seen)\n",
    "    '''elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum([get_size(i, seen) for i in obj])'''\n",
    "    return size\n",
    "\n",
    "#%%\n",
    "def LF_experiment(train_x, train_y, test_x, test_y, ntrees, shift, slot, model, num_points_per_task, acorn=None):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    single_task_accuracies = np.zeros(10,dtype=float)\n",
    "    shifts = []\n",
    "    tasks = []\n",
    "    base_tasks = []\n",
    "    accuracies_across_tasks = []\n",
    "    train_times_across_tasks = []\n",
    "    single_task_inference_times_across_tasks = []\n",
    "    multitask_inference_times_across_tasks = []\n",
    "    time_info = []\n",
    "    mem_info = []\n",
    "\n",
    "    if model == \"dnn\":\n",
    "        default_transformer_class = NeuralClassificationTransformer\n",
    "\n",
    "        network = keras.Sequential()\n",
    "        network.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu', input_shape=np.shape(train_x)[1:]))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Conv2D(filters=32, kernel_size=(3, 3), strides = 2, padding = \"same\", activation='relu'))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Conv2D(filters=64, kernel_size=(3, 3), strides = 2, padding = \"same\", activation='relu'))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides = 2, padding = \"same\", activation='relu'))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Conv2D(filters=254, kernel_size=(3, 3), strides = 2, padding = \"same\", activation='relu'))\n",
    "\n",
    "        network.add(layers.Flatten())\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Dense(2000, activation='relu'))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Dense(2000, activation='relu'))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Dense(units=10, activation = 'softmax'))\n",
    "\n",
    "        default_transformer_kwargs = {\n",
    "            \"network\": network,\n",
    "            \"euclidean_layer_idx\": -2,\n",
    "            \"loss\": losses.contrastive_loss(X_train, labels=y_train),\n",
    "            \"optimizer\": Adam(3e-4),\n",
    "            \"fit_kwargs\": {\n",
    "                \"epochs\": 100,\n",
    "                \"callbacks\": [EarlyStopping(patience=5, monitor=\"val_loss\")],\n",
    "                \"verbose\": False,\n",
    "                \"validation_split\": 0.33,\n",
    "                \"batch_size\": 32,\n",
    "            },\n",
    "        }\n",
    "        default_voter_class = KNNClassificationVoter\n",
    "        default_voter_kwargs = {\"k\" : int(np.log2(num_points_per_task))}\n",
    "\n",
    "        default_decider_class = SimpleArgmaxAverage\n",
    "    elif model == \"uf\":\n",
    "        default_transformer_class = TreeClassificationTransformer\n",
    "        default_transformer_kwargs = {\"kwargs\" : {\"max_depth\" : 30, \"max_features\" : \"auto\"}}\n",
    "\n",
    "        default_voter_class = TreeClassificationVoter\n",
    "        default_voter_kwargs = {}\n",
    "\n",
    "        default_decider_class = SimpleArgmaxAverage\n",
    "\n",
    "\n",
    "    progressive_learner = ProgressiveLearner(default_transformer_class = default_transformer_class,\n",
    "                                         default_transformer_kwargs = default_transformer_kwargs,\n",
    "                                         default_voter_class = default_voter_class,\n",
    "                                         default_voter_kwargs = default_voter_kwargs,\n",
    "                                         default_decider_class = default_decider_class)\n",
    "\n",
    "    for task_ii in range(10):\n",
    "        print(\"Starting Task {} For Fold {}\".format(task_ii, shift))\n",
    "\n",
    "\n",
    "        train_start_time = time.time()\n",
    "        \n",
    "        if acorn is not None:\n",
    "            np.random.seed(acorn)\n",
    "\n",
    "        progressive_learner.add_task(\n",
    "            X = train_x[task_ii*5000+slot*num_points_per_task:task_ii*5000+(slot+1)*num_points_per_task],\n",
    "            y = train_y[task_ii*5000+slot*num_points_per_task:task_ii*5000+(slot+1)*num_points_per_task],\n",
    "            num_transformers = 1 if model == \"dnn\" else ntrees,\n",
    "            transformer_voter_decider_split = [0.63, 0.37, 0],\n",
    "            decider_kwargs = {\"classes\" : np.unique(train_y[task_ii*5000+slot*num_points_per_task:task_ii*5000+(slot+1)*num_points_per_task])}\n",
    "            )\n",
    "        train_end_time = time.time()\n",
    "        \n",
    "        single_learner = ProgressiveLearner(default_transformer_class = default_transformer_class,\n",
    "                                         default_transformer_kwargs = default_transformer_kwargs,\n",
    "                                         default_voter_class = default_voter_class,\n",
    "                                         default_voter_kwargs = default_voter_kwargs,\n",
    "                                         default_decider_class = default_decider_class)\n",
    "\n",
    "        if acorn is not None:\n",
    "            np.random.seed(acorn)\n",
    "\n",
    "        single_learner.add_task(\n",
    "            X = train_x[task_ii*5000+slot*num_points_per_task:task_ii*5000+(slot+1)*num_points_per_task],\n",
    "            y = train_y[task_ii*5000+slot*num_points_per_task:task_ii*5000+(slot+1)*num_points_per_task],\n",
    "            num_transformers = 1 if model == \"dnn\" else (task_ii+1)*ntrees,\n",
    "            transformer_voter_decider_split = [0.67, 0.33, 0],\n",
    "            decider_kwargs = {\"classes\" : np.unique(train_y[task_ii*5000+slot*num_points_per_task:task_ii*5000+(slot+1)*num_points_per_task])}\n",
    "            )\n",
    "\n",
    "        time_info.append(train_end_time - train_start_time)\n",
    "        mem_info.append(get_size(progressive_learner))\n",
    "        train_times_across_tasks.append(train_end_time - train_start_time)\n",
    "\n",
    "        single_task_inference_start_time = time.time()\n",
    "        single_task=single_learner.predict(\n",
    "            X = test_x[task_ii*1000:(task_ii+1)*1000,:], transformer_ids=[0], task_id=0\n",
    "            )\n",
    "        single_task_inference_end_time = time.time()\n",
    "        single_task_accuracies[task_ii] = np.mean(\n",
    "                single_task == test_y[task_ii*1000:(task_ii+1)*1000]\n",
    "                    )\n",
    "        single_task_inference_times_across_tasks.append(single_task_inference_end_time - single_task_inference_start_time)\n",
    "\n",
    "\n",
    "\n",
    "        for task_jj in range(task_ii+1):\n",
    "            multitask_inference_start_time = time.time()\n",
    "            llf_task=progressive_learner.predict(\n",
    "                X = test_x[task_jj*1000:(task_jj+1)*1000,:], task_id=task_jj\n",
    "                )\n",
    "            multitask_inference_end_time = time.time()\n",
    "\n",
    "            shifts.append(shift)\n",
    "            tasks.append(task_jj+1)\n",
    "            base_tasks.append(task_ii+1)\n",
    "            accuracies_across_tasks.append(np.mean(\n",
    "                llf_task == test_y[task_jj*1000:(task_jj+1)*1000]\n",
    "                ))\n",
    "            multitask_inference_times_across_tasks.append(multitask_inference_end_time - multitask_inference_start_time)\n",
    "\n",
    "    df['data_fold'] = shifts\n",
    "    df['task'] = tasks\n",
    "    df['base_task'] = base_tasks\n",
    "    df['accuracy'] = accuracies_across_tasks\n",
    "    df['multitask_inference_times'] = multitask_inference_times_across_tasks\n",
    "\n",
    "    df_single_task = pd.DataFrame()\n",
    "    df_single_task['task'] = range(1, 11)\n",
    "    df_single_task['data_fold'] = shift\n",
    "    df_single_task['accuracy'] = single_task_accuracies\n",
    "    df_single_task['single_task_inference_times'] = single_task_inference_times_across_tasks\n",
    "    df_single_task['train_times'] = train_times_across_tasks\n",
    "\n",
    "    #print(df)\n",
    "    summary = (df,df_single_task)\n",
    "    file_to_save = 'C:/Users/walee/Desktop/JHU Course Material/Semester 5 (Fall 2021)/NDD 1/ProgLearn/benchmarks/cifar_exp/result/result/'+model+str(ntrees)+'_'+str(shift)+'_Adam'+'.pickle'\n",
    "    with open(file_to_save, 'wb') as f:\n",
    "        pickle.dump(summary, f)\n",
    "\n",
    "    '''file_to_save = 'result/time_res/'+model+str(ntrees)+'_'+str(shift)+'_'+str(slot)+'.pickle'\n",
    "    with open(file_to_save, 'wb') as f:\n",
    "        pickle.dump(time_info, f)\n",
    "    file_to_save = 'result/mem_res/'+model+str(ntrees)+'_'+str(shift)+'_'+str(slot)+'.pickle'\n",
    "    with open(file_to_save, 'wb') as f:\n",
    "        pickle.dump(mem_info, f)'''\n",
    "\n",
    "#%%\n",
    "def cross_val_data(data_x, data_y, num_points_per_task, total_task=10, shift=1):\n",
    "    x = data_x.copy()\n",
    "    y = data_y.copy()\n",
    "    idx = [np.where(data_y == u)[0] for u in np.unique(data_y)]\n",
    "\n",
    "    batch_per_task=5000//num_points_per_task\n",
    "    sample_per_class = num_points_per_task//total_task\n",
    "    test_data_slot=100//batch_per_task\n",
    "\n",
    "    for task in range(total_task):\n",
    "        for batch in range(batch_per_task):\n",
    "            for class_no in range(task*10,(task+1)*10,1):\n",
    "                indx = np.roll(idx[class_no],(shift-1)*100)\n",
    "\n",
    "                if batch==0 and class_no==0 and task==0:\n",
    "                    train_x = x[indx[batch*sample_per_class:(batch+1)*sample_per_class],:]\n",
    "                    train_y = y[indx[batch*sample_per_class:(batch+1)*sample_per_class]]\n",
    "                    test_x = x[indx[batch*test_data_slot+500:(batch+1)*test_data_slot+500],:]\n",
    "                    test_y = y[indx[batch*test_data_slot+500:(batch+1)*test_data_slot+500]]\n",
    "                else:\n",
    "                    train_x = np.concatenate((train_x, x[indx[batch*sample_per_class:(batch+1)*sample_per_class],:]), axis=0)\n",
    "                    train_y = np.concatenate((train_y, y[indx[batch*sample_per_class:(batch+1)*sample_per_class]]), axis=0)\n",
    "                    test_x = np.concatenate((test_x, x[indx[batch*test_data_slot+500:(batch+1)*test_data_slot+500],:]), axis=0)\n",
    "                    test_y = np.concatenate((test_y, y[indx[batch*test_data_slot+500:(batch+1)*test_data_slot+500]]), axis=0)\n",
    "\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "#%%\n",
    "def run_parallel_exp(data_x, data_y, n_trees, model, num_points_per_task, slot=0, shift=1):\n",
    "    train_x, train_y, test_x, test_y = cross_val_data(data_x, data_y, num_points_per_task, shift=shift)\n",
    "\n",
    "    if model == \"dnn\":\n",
    "        config = tf.compat.v1.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        sess = tf.compat.v1.Session(config=config)\n",
    "        with tf.device('/gpu:'+str(shift % 4)):\n",
    "            LF_experiment(train_x, train_y, test_x, test_y, n_trees, shift, slot, model, num_points_per_task, acorn=12345)\n",
    "    else:\n",
    "        LF_experiment(train_x, train_y, test_x, test_y, n_trees, shift, slot, model, num_points_per_task, acorn=12345)\n",
    "\n",
    "#%%\n",
    "### MAIN HYPERPARAMS ###\n",
    "model = \"dnn\"\n",
    "num_points_per_task = 500 # change from 5000 to 500\n",
    "########################\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "data_x = np.concatenate([X_train, X_test])\n",
    "if model == \"uf\":\n",
    "    data_x = data_x.reshape((data_x.shape[0], data_x.shape[1] * data_x.shape[2] * data_x.shape[3]))\n",
    "data_y = np.concatenate([y_train, y_test])\n",
    "data_y = data_y[:, 0]\n",
    "\n",
    "\n",
    "#%%\n",
    "if model == \"uf\":\n",
    "    slot_fold = range(1)\n",
    "    shift_fold = range(1,7,1)\n",
    "    n_trees=[10]\n",
    "    iterable = product(n_trees,shift_fold,slot_fold)\n",
    "    Parallel(n_jobs=-2,verbose=1)(\n",
    "        delayed(run_parallel_exp)(\n",
    "                data_x, data_y, ntree, model, num_points_per_task, slot=slot, shift=shift\n",
    "                ) for ntree,shift,slot in iterable\n",
    "                )\n",
    "elif model == \"dnn\":\n",
    "    slot_fold = range(10) #edit this default 10 is correct?\n",
    "    \n",
    "    '''\n",
    "    #parallel\n",
    "    def perform_shift(shift_slot_tuple):\n",
    "        shift, slot = shift_slot_tuple\n",
    "        return run_parallel_exp(data_x, data_y, 0, model, num_points_per_task, slot=slot, shift=shift)\n",
    "    print(\"Performing Stage 1 Shifts\")\n",
    "    stage_1_shifts = range(1, 5)\n",
    "    stage_1_iterable = product(stage_1_shifts,slot_fold)\n",
    "    with Pool(4) as p:\n",
    "        p.map(perform_shift, stage_1_iterable)\n",
    "    print(\"Performing Stage 2 Shifts\")\n",
    "    stage_2_shifts = range(5, 7)\n",
    "    stage_2_iterable = product(stage_2_shifts,slot_fold)\n",
    "    with Pool(4) as p:\n",
    "        p.map(perform_shift, stage_2_iterable)\n",
    "    '''\n",
    "\n",
    "    #sequential\n",
    "    #slot_fold = range(1) #this should be 10, comment out\n",
    "    shift_fold = [1,2,3,4,5,6]\n",
    "    n_trees=[0]\n",
    "    iterable = product(n_trees,shift_fold,slot_fold)\n",
    "\n",
    "    for ntree,shift,slot in iterable:\n",
    "        run_parallel_exp(\n",
    "                    data_x, data_y, ntree, model, num_points_per_task, slot=slot, shift=shift\n",
    "                    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
